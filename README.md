# üß† Tech Challenge - Fine-tuning de Modelos de Linguagem com AmazonTitles-1.3MM

Este projeto foi desenvolvido como parte do **Tech Challenge da FIAP (Fase 3 ‚Äì Intelig√™ncia Artificial)** e tem como objetivo aplicar t√©cnicas de **fine-tuning em modelos fundacionais** (como LLaMA, TinyLLaMA, Mistral, etc.) utilizando o dataset **AmazonTitles-1.3MM**.

O prop√≥sito √© treinar um modelo capaz de **gerar descri√ß√µes de produtos a partir de seus t√≠tulos**, simulando perguntas reais de usu√°rios sobre itens dispon√≠veis no cat√°logo da Amazon.

---

## üìÅ Estrutura do Projeto

```
Tech-Challenge-AmazonTitles/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ trn.json                 # Dataset original (fonte AmazonTitles-1.3MM) baixa e adicionar nesta pasta
‚îÇ   ‚îî‚îÄ‚îÄ amazon_sft.jsonl         # Dataset preparado para fine-tuning
‚îú‚îÄ‚îÄ out/
‚îÇ   ‚îî‚îÄ‚îÄ tinyllama-lora/          # Diret√≥rio do adapter salvo ap√≥s o fine-tuning
‚îú‚îÄ‚îÄ prep_data.py                 # Script de pr√©-processamento e prepara√ß√£o do dataset
‚îú‚îÄ‚îÄ eval_baseline.py             # Avalia√ß√£o do modelo base antes do treinamento
‚îú‚îÄ‚îÄ train_sft.py                 # Execu√ß√£o do fine-tuning (LoRA/QLoRA)
‚îú‚îÄ‚îÄ inference.py                 # Gera√ß√£o de respostas com o modelo treinado
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md                    # Documenta√ß√£o do projeto
```

---

## üìä Sobre o Dataset

O **AmazonTitles-1.3MM** cont√©m consultas e t√≠tulos de produtos da Amazon associados √†s suas descri√ß√µes, coletados a partir de intera√ß√µes reais de usu√°rios.

Cada entrada do arquivo `trn.json` possui o formato:
```json
{
  "uid": "0000031909",
  "title": "Girls Ballet Tutu Neon Pink",
  "content": "High quality 3 layer ballet tutu. 12 inches in length",
  "target_ind": [...],
  "target_rel": [...]
}
```

Para o fine-tuning, s√£o utilizadas apenas as colunas:
- **title** ‚Üí t√≠tulo do produto  
- **content** ‚Üí descri√ß√£o (texto alvo)

Essas informa√ß√µes s√£o transformadas em prompts de entrada para treinar o modelo a responder perguntas como:
> "Quais s√£o as principais caracter√≠sticas e benef√≠cios deste produto?"

---

## ‚öôÔ∏è Como Executar o Projeto

### 1Ô∏è‚É£ Preparar os Dados
```powershell
python .\prep_data.py `
  --input "C:\Users\vkrlo\OneDrive\√Årea de Trabalho\Tech-Challenge-3-IA-FIAP\data\trn.json" `
  --output .\data\amazon_sft.jsonl `
  --variants-per-title 2 `
  --max-samples 200000 `
  --min-len 10 `
  --max-content-len 1200
```

### 2Ô∏è‚É£ Avaliar o Modelo Base (pr√©-treino)
```powershell
python .\eval_baseline.py `
  --model-id TinyLlama/TinyLlama-1.1B-Chat-v1.0 `
  --title "Fone de Ouvido Bluetooth JBL Tune 510BT" `
  --question "Quais s√£o as principais caracter√≠sticas e benef√≠cios?"
```

### 3Ô∏è‚É£ Executar o Fine-Tuning
```powershell
python .\train_sft.py `
  --model-id TinyLlama/TinyLlama-1.1B-Chat-v1.0 `
  --dataset-path .\data\amazon_sft.jsonl `
  --out .\out\tinyllama-lora `
  --epochs 1 --seq-len 1024 --batch 2 --grad-accum 8
```

### 4Ô∏è‚É£ Fazer Infer√™ncia
```powershell
python .\inference.py `
  --model-id TinyLlama/TinyLlama-1.1B-Chat-v1.0 `
  --adapter .\out\tinyllama-lora `
  --title "Fone de Ouvido Bluetooth JBL Tune 510BT" `
  --question "Quais s√£o as principais caracter√≠sticas e benef√≠cios?" `
  --max-new-tokens 420 `
  --device-map cpu `
  --force-pt
```

### 5Ô∏è‚É£ Executar Interface Web (opcional)
```powershell
python .\app.py
```

---

## üß† T√©cnicas Utilizadas

- **Fine-Tuning Supervisionado (SFT)** com LoRA / QLoRA  
- **Modelos base compat√≠veis com Hugging Face Transformers**
- **Tokeniza√ß√£o e truncamento din√¢mico**
- **Avalia√ß√£o baseline antes do treino**
- **Infer√™ncia com tradu√ß√£o autom√°tica para PT-BR**
- **Offload autom√°tico para CPU (compat√≠vel com Windows)**

---

## ‚ö° Dicas de Execu√ß√£o

- No Windows, se o `bitsandbytes` n√£o estiver dispon√≠vel, use:
  ```bash
  --optim adamw_torch
  ```
- Ajuste `--seq-len`, `--batch` e `--grad-accum` conforme o limite de mem√≥ria.
- Para rodar sem GPU, adicione `--device-map cpu` e `--offload-dir offload_infer`.

---

## üì¶ Sa√≠das Geradas

- **`out/tinyllama-lora/`** ‚Üí Adapter do modelo fine-tunado.  
- **`data/amazon_sft.jsonl`** ‚Üí Dataset formatado para treinamento.  
- **Respostas inferidas** ‚Üí Sa√≠da textual em portugu√™s (via `--force-pt`).

---

## üìö Bibliotecas Principais

- `transformers` ‚Äì Modelos fundacionais e tokeniza√ß√£o  
- `datasets` ‚Äì Manipula√ß√£o e split de dados  
- `trl` ‚Äì Fine-tuning supervisionado (SFTTrainer)  
- `peft` ‚Äì Adapta√ß√£o leve com LoRA / QLoRA  
- `accelerate` ‚Äì Treinamento otimizado (CPU/GPU/offload)  
- `torch` ‚Äì Backend de deep learning  

---

## üë®‚Äçüíª Autor

**Phellype Guilherme Pereira da Silva**  
**RM:** 361625  
**Projeto:** Fase 3 - P√≥s Tech FIAP - Intelig√™ncia Artificial  
**Institui√ß√£o:** [FIAP - Faculdade de Inform√°tica e Administra√ß√£o Paulista](https://www.fiap.com.br)
